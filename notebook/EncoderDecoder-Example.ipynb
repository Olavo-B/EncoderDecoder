{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VlcDlXwd9LS"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KNv_S-4SSPz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import sklearn\n",
        "\n",
        "# Step 1: Custom Dataset Class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data (np.ndarray): A numpy array of shape (num_samples, input_dim).\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return torch.tensor(sample, dtype=torch.float32)\n",
        "\n",
        "# Step 2: Generate Example Data\n",
        "def generate_dummy_data(num_samples=1000, input_dim=10):\n",
        "    \"\"\"Generates dummy data for testing the dataloader.\"\"\"\n",
        "    return sklearn.datasets.make_classification(\n",
        "        n_samples=num_samples, n_features=input_dim, \n",
        "        n_informative=input_dim, n_redundant=0, random_state=42)\n",
        "\n",
        "# Step 3: Transformations (Optional)\n",
        "def normalize_data(sample):\n",
        "    \"\"\"Normalize the sample to have values between 0 and 1.\"\"\"\n",
        "    return sample / np.max(sample)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(data_path):\n",
        "    import pandas as pd\n",
        "    # change first column name to 'target'\n",
        "    df = pd.read_csv(data_path)\n",
        "    df.rename(columns={df.columns[0]: 'target'}, inplace=True)\n",
        "    data = df.drop('target', axis=1).values\n",
        "    target = df['target'].values\n",
        "\n",
        "        \n",
        "\n",
        "    return data,target\n",
        "\n",
        "def save_data(data, data_path):\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(data_path, index=False)\n",
        "    print(f\"Data saved to {data_path}\")\n",
        "\n",
        "def preprocess(input_path, output_path):\n",
        "    import random\n",
        "    import pandas as pd\n",
        "    # Implementation of preprocessing logic\n",
        "    data,target = load_data(input_path)\n",
        "\n",
        "    # Preprocess data\n",
        "    # Get 3 random columns from data and concatenate them to target\n",
        "    random.seed(0)\n",
        "    indices = random.sample(range(data.shape[1]), 3)\n",
        "    x = np.concatenate([data[:, indices], target.reshape(-1, 1)], axis=1)\n",
        "\n",
        "    # Save preprocessed data\n",
        "    save_data(x, output_path)\n",
        "\n",
        "    return x\n",
        "    \n",
        "    print(f\"Preprocessing {input_path} -> {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/app/notebook\n"
          ]
        }
      ],
      "source": [
        "SAVE_FOLDER = 'data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Download Susy Dataset: 424MiB [02:29, 3.15MiB/s] "
          ]
        }
      ],
      "source": [
        "# Step 4: Initialize Dataset and DataLoader\n",
        "from LoadDataset import LoadDataset\n",
        "_,_ = LoadDataset.load_iris(save_path=SAVE_FOLDER)\n",
        "data = preprocess(f'{SAVE_FOLDER}/iris.csv', f'{SAVE_FOLDER}/iris_preprocessed.csv')\n",
        "dataset = CustomDataset(data, transform=normalize_data)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stx63b2LeBl9"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "x1wMYsnteShM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, context_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        layers = []\n",
        "        dims = [input_dim] + hidden_dims + [context_dim]\n",
        "        for i in range(len(dims) - 1):\n",
        "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "            if i < len(dims) - 2:  # No activation in the final layer\n",
        "                layers.append(nn.ReLU())\n",
        "        self.encoder = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgcHkH2KeDV4"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "i7RmHQyEeVuX"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, context_dim, hidden_dims, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        layers = []\n",
        "        dims = [context_dim] + hidden_dims + [output_dim]\n",
        "        for i in range(len(dims) - 1):\n",
        "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "            if i < len(dims) - 2:  # No activation in the final layer\n",
        "                layers.append(nn.ReLU())\n",
        "        self.decoder = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PytVAQAdeFPd"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verify if has cuda\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fVKt0NgeY3N",
        "outputId": "03e19629-6844-42b6-b139-75168e102b2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.32914501428604126\n",
            "Epoch 2, Loss: 0.2802676260471344\n",
            "Epoch 3, Loss: 0.2335808426141739\n",
            "Epoch 4, Loss: 0.18161512911319733\n",
            "Epoch 5, Loss: 0.13997283577919006\n",
            "Epoch 6, Loss: 0.10327020287513733\n",
            "Epoch 7, Loss: 0.06271777302026749\n",
            "Epoch 8, Loss: 0.031497787684202194\n",
            "Epoch 9, Loss: 0.014579257927834988\n",
            "Epoch 10, Loss: 0.0156423207372427\n",
            "Epoch 11, Loss: 0.012110820040106773\n",
            "Epoch 12, Loss: 0.014665823429822922\n",
            "Epoch 13, Loss: 0.01096057053655386\n",
            "Epoch 14, Loss: 0.009064801968634129\n",
            "Epoch 15, Loss: 0.008202067576348782\n",
            "Epoch 16, Loss: 0.008607006631791592\n",
            "Epoch 17, Loss: 0.007492826785892248\n",
            "Epoch 18, Loss: 0.010673868469893932\n",
            "Epoch 19, Loss: 0.00989864394068718\n",
            "Epoch 20, Loss: 0.010322686284780502\n",
            "Epoch 21, Loss: 0.011166345328092575\n",
            "Epoch 22, Loss: 0.01033459510654211\n",
            "Epoch 23, Loss: 0.009935668669641018\n",
            "Epoch 24, Loss: 0.010846325196325779\n",
            "Epoch 25, Loss: 0.009007207117974758\n",
            "Epoch 26, Loss: 0.008254838176071644\n",
            "Epoch 27, Loss: 0.008963996544480324\n",
            "Epoch 28, Loss: 0.010301491245627403\n",
            "Epoch 29, Loss: 0.010598655790090561\n",
            "Epoch 30, Loss: 0.010796756483614445\n",
            "Epoch 31, Loss: 0.00853580329567194\n",
            "Epoch 32, Loss: 0.008420365862548351\n",
            "Epoch 33, Loss: 0.006136447656899691\n",
            "Epoch 34, Loss: 0.007265135645866394\n",
            "Epoch 35, Loss: 0.005611693486571312\n",
            "Epoch 36, Loss: 0.0028958963230252266\n",
            "Epoch 37, Loss: 0.0036849183961749077\n",
            "Epoch 38, Loss: 0.0032202275469899178\n",
            "Epoch 39, Loss: 0.0029053085017949343\n",
            "Epoch 40, Loss: 0.0023362119682133198\n",
            "Epoch 41, Loss: 0.0015307411085814238\n",
            "Epoch 42, Loss: 0.0017685096245259047\n",
            "Epoch 43, Loss: 0.0016177776269614697\n",
            "Epoch 44, Loss: 0.0008692366536706686\n",
            "Epoch 45, Loss: 0.0011406374396756291\n",
            "Epoch 46, Loss: 0.0015487541677430272\n",
            "Epoch 47, Loss: 0.0011462412076070905\n",
            "Epoch 48, Loss: 0.0009065428166650236\n",
            "Epoch 49, Loss: 0.0008396515040658414\n",
            "Epoch 50, Loss: 0.0007608100422658026\n",
            "Epoch 51, Loss: 0.0010560767259448767\n",
            "Epoch 52, Loss: 0.0011390337022021413\n",
            "Epoch 53, Loss: 0.0013388297520577908\n",
            "Epoch 54, Loss: 0.0010205524740740657\n",
            "Epoch 55, Loss: 0.0010456036543473601\n",
            "Epoch 56, Loss: 0.0009160386398434639\n",
            "Epoch 57, Loss: 0.001108944183215499\n",
            "Epoch 58, Loss: 0.0007895054295659065\n",
            "Epoch 59, Loss: 0.0014081947738304734\n",
            "Epoch 60, Loss: 0.0008536680252291262\n",
            "Epoch 61, Loss: 0.0009304119739681482\n",
            "Epoch 62, Loss: 0.0007195291691459715\n",
            "Epoch 63, Loss: 0.0009095318964682519\n",
            "Epoch 64, Loss: 0.0013112309388816357\n",
            "Epoch 65, Loss: 0.0011812546290457249\n",
            "Epoch 66, Loss: 0.0008984412997961044\n",
            "Epoch 67, Loss: 0.0011616612318903208\n",
            "Epoch 68, Loss: 0.0008849012083373964\n",
            "Epoch 69, Loss: 0.0014559644041582942\n",
            "Epoch 70, Loss: 0.0009982361225411296\n",
            "Epoch 71, Loss: 0.0011517623206600547\n",
            "Epoch 72, Loss: 0.0009590839617885649\n",
            "Epoch 73, Loss: 0.0008508771425113082\n",
            "Epoch 74, Loss: 0.000715411442797631\n",
            "Epoch 75, Loss: 0.0008320873603224754\n",
            "Epoch 76, Loss: 0.0003898918512277305\n",
            "Epoch 77, Loss: 0.0007630407926626503\n",
            "Epoch 78, Loss: 0.00112853420432657\n",
            "Epoch 79, Loss: 0.0006224697572179139\n",
            "Epoch 80, Loss: 0.000548803189303726\n",
            "Epoch 81, Loss: 0.0005561787402257323\n",
            "Epoch 82, Loss: 0.00033639694447629154\n",
            "Epoch 83, Loss: 0.0008925474830903113\n",
            "Epoch 84, Loss: 0.0010342254536226392\n",
            "Epoch 85, Loss: 0.0009215585305355489\n",
            "Epoch 86, Loss: 0.0007518109050579369\n",
            "Epoch 87, Loss: 0.000962655758485198\n",
            "Epoch 88, Loss: 0.0008754367008805275\n",
            "Epoch 89, Loss: 0.0007985690026544034\n",
            "Epoch 90, Loss: 0.0007080903160385787\n",
            "Epoch 91, Loss: 0.0006574565195478499\n",
            "Epoch 92, Loss: 0.0008899848908185959\n",
            "Epoch 93, Loss: 0.0008277893648482859\n",
            "Epoch 94, Loss: 0.0008355873287655413\n",
            "Epoch 95, Loss: 0.0008884461130946875\n",
            "Epoch 96, Loss: 0.0004806090146303177\n",
            "Epoch 97, Loss: 0.0006266600685194135\n",
            "Epoch 98, Loss: 0.0009421056602150202\n",
            "Epoch 99, Loss: 0.0005982888396829367\n",
            "Epoch 100, Loss: 0.0005251984111964703\n"
          ]
        }
      ],
      "source": [
        "encoder = Encoder(input_dim=4, hidden_dims=[64, 32], context_dim=2)\n",
        "decoder = Decoder(context_dim=2, hidden_dims=[32, 64], output_dim=4)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
        "\n",
        "# Example Training Loop\n",
        "for epoch in range(100):\n",
        "    for batch in dataloader:\n",
        "        x = batch  # Assuming x is your input data\n",
        "        context = encoder(x)\n",
        "        output = decoder(context)\n",
        "        loss = criterion(output, x)  # Reconstruction loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCl-7myJfAlz"
      },
      "source": [
        "# Visualize Context Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "id": "bzdtg-XlfGxB",
        "outputId": "622a672c-7801-42f8-93ed-d89bc98cce97"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "n_components=2 must be between 0 and min(n_samples, n_features)=1 with svd_solver='covariance_eigh'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[41], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     36\u001b[0m context_vectors \u001b[38;5;241m=\u001b[39m extract_context_vectors(encoder, dataloader)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mvisualize_context_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpca\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[41], line 28\u001b[0m, in \u001b[0;36mvisualize_context_vectors\u001b[0;34m(context_vectors, method)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpca\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsne\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m reduced_context \u001b[38;5;241m=\u001b[39m \u001b[43mreducer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_vectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     30\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(reduced_context[:, \u001b[38;5;241m0\u001b[39m], reduced_context[:, \u001b[38;5;241m1\u001b[39m], alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:468\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    447\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 468\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:542\u001b[0m, in \u001b[0;36mPCA._fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_eigh\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, xp)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/decomposition/_pca.py:556\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[0;34m(self, X, n_components, xp, is_array_api_compliant)\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is only supported if n_samples >= n_features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    554\u001b[0m         )\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m     )\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: n_components=2 must be between 0 and min(n_samples, n_features)=1 with svd_solver='covariance_eigh'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def extract_context_vectors(encoder, dataloader):\n",
        "    \"\"\"Extracts context vectors from the encoder for the entire dataset.\"\"\"\n",
        "    context_vectors = []\n",
        "    encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            context = encoder(batch)\n",
        "            context_vectors.append(context.numpy())\n",
        "    return np.vstack(context_vectors)\n",
        "\n",
        "\n",
        "def visualize_context_vectors(context_vectors, method=\"pca\"):\n",
        "    \"\"\"Visualizes context vectors using PCA or t-SNE.\"\"\"\n",
        "    if method == \"pca\":\n",
        "        reducer = PCA(n_components=2)\n",
        "    elif method == \"tsne\":\n",
        "        reducer = TSNE(n_components=2, random_state=42)\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'pca' or 'tsne'\")\n",
        "\n",
        "    reduced_context = reducer.fit_transform(context_vectors)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(reduced_context[:, 0], reduced_context[:, 1], alpha=0.7, s=20, cmap='viridis')\n",
        "    plt.title(f\"Context Vectors Visualization ({method.upper()})\")\n",
        "    plt.xlabel(\"Component 1\")\n",
        "    plt.ylabel(\"Component 2\")\n",
        "    plt.show()\n",
        "\n",
        "context_vectors = extract_context_vectors(encoder, dataloader)\n",
        "visualize_context_vectors(context_vectors, method=\"pca\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED6f6hCsgXbL",
        "outputId": "5fcdeab4-19bb-455e-ec4e-ea1ea326569a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(150, 1)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context_vectors.shape"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
